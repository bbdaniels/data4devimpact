%------------------------------------------------

\begin{fullwidth}
Transforming raw data into a substantial contribution to scientific knowledge
requires a mix of subject expertise, programming skills,
and statistical and econometric knowledge.
The process of data analysis is typically
a back-and-forth discussion between people
with differing skill sets.
An essential part of the process is translating the
raw data received from the field into economically meaningful indicators.
To effectively do this in a team environment,
data, code and outputs must be well-organized,
with a clear system for version control,
and analytical scripts structured such that any member of the research team can run them.
Putting in time upfront to structure data work well
pays substantial dividends throughout the process.

In this chapter, we first cover data management:
how to organize your data work at the start of a project
so that coding the analysis itself is straightforward.
This includes setting up folders, organizing tasks, master scripts,
and putting in place a version control system
so that your work is easy for all research team members to follow,
and meets standards for transparency and reproducibility.
Second, we turn to de-identification,
a critical step when working with any personally-identified data.
In the third section, we offer detailed guidance on data cleaning,
from identifying duplicate entries to labeling and annotating raw data,
and how to transparently document the cleaning process.
Section four focuses on how to transform your clean data
into the actual indicators you will need for analysis,
again emphasizing the importance of transparent documentation.
Finally, we turn to analysis itself.
We do not offer instructions on how to conduct specific analyses,
as that is determined by research design;
rather, we discuss how to structure analysis code,
and how to automate common outputs so that your analysis is fully reproducible.

\end{fullwidth}

%------------------------------------------------

\section{Managing data effectively}

The goal of data management is to organize the components of data work
so the complete process can traced, understood, and revised without massive effort.
We focus on four key elements to good data management:
folder structure, task breakdown, master scripts, and version control.
A good \textbf{folder structure} organizes files so that any material can be found when needed.
It reflects a \textbf{task breakdown} into steps with well-defined inputs, tasks, and outputs.
A \textbf{master script} connects folder structure and code.
It is a one-file summary of your whole project.
Finally, \textbf{version control} gives you clear file histories and backups,
which enable the team to edit files without fear of losing information
and track how each edit affects other files in the project.

\subsection{Organizing your folder structure}

There are many ways to organize research data.
\index{data organization}
Our team at DIME Analytics developed the \texttt{iefolder}\sidenote{
	\url{https://dimewiki.worldbank.org/iefolder}}
command (part of \texttt{ietoolkit}\sidenote{
	\url{https://dimewiki.worldbank.org/ietoolkit}})
to automatize the creation of folders following our preferred scheme and
to standardize folder structures across teams and projects.
A standardized structure greatly reduces the costs that PIs and RAs
face when switching between projects,
because folders are organized in exactly the same way
and use the same file paths, shortcuts, and macro references.\sidenote{
	\url{https://dimewiki.worldbank.org/DataWork\_Folder}}
We created \texttt{iefolder} based on our experience with survey data,
but it can be used for other types of data.
Other teams may prefer a different scheme, but
the principle of creating a single unified standard remains.

At the top level of the structure created by \texttt{iefolder} are what we call ``round'' folders.\sidenote{
	\url{https://dimewiki.worldbank.org/DataWork\_Survey\_Round}}
You can think of a ``round'' as a single source of data,
which will all be cleaned using a single script.
Inside each round folder, there are dedicated folders for:
raw (encrypted) data; de-identified data; cleaned data; and final (constructed) data.
There is a folder for raw results, as well as for final outputs.
The folders that hold code are organized in parallel to these,
so that the progression through the whole project can be followed by anyone new to the team.
Additionally, \texttt{iefolder} creates \textbf{master do-files}\sidenote{
	\url{https://dimewiki.worldbank.org/Master\_Do-files}}
so the structure of all project code is reflected in a top-level script.

\subsection{Breaking down tasks}

We divide the process of transforming raw datasets to research outputs into 
four steps:
de-identification, data cleaning, variable construction, and data analysis.
Though they are frequently implemented concurrently,
creating separate scripts and datasets prevents mistakes.
It will be easier to understand this division as we discuss what each stage comprises.
What you should know for now is that each of these stages has well-defined inputs and outputs.
This makes it easier to track tasks across scripts,
and avoids duplication of code that could lead to inconsistent results.
For each stage, there should be a code folder and a corresponding dataset.
The names of codes, datasets and outputs for each stage should be consistent,
making clear how they relate to one another.
So, for example, a script called \texttt{section-1-cleaning} would create
a dataset called \texttt{section-1-clean}.

The division of a project in stages also facilitates a review workflow inside your team.
The code, data and outputs of each of these stages should go through at least one round of code review,
in which team members read and run each other's codes.
Reviewing code at each stage, rather than waiting until the end of a project,
is preferrable as the amount of code to review is more manageable and
it allows you to correct errors in real-time (e.g. correcting errors in variable construction before analysis begins).
Code review is a common quality assurance practice among data scientists.
It helps to keep the quality of the outputs high, and is also a great way to learn and improve your own code.

\subsection{Writing master scripts}

Master scripts allow users to execute all the project code from a single file.
As discussed in Chapter 2, the master script should briefly describe what each
section of the code does, and map the files they require and create.
The master script also connects code and folder structure through macros or objects.
In short, a master script is a human-readable map of the tasks,
files, and folder structure that comprise a project.
Having a master script eliminates the need for complex instructions to replicate results.
Reading it should be enough for anyone unfamiliar with the project
to understand what are the main tasks, which scripts execute them,
and where different files can be found in the project folder.
That is, it should contain all the information needed to interact with a project's data work.

\subsection{Implementing version control}

Establishing a version control system is an incredibly useful
and important step for documentation, collaboration and conflict-solving.
Version control allows you to effectively track code edits,
including the addition and deletion of files.
This way you can delete code you no longer need,
and still recover it easily if you ever need to get back previous work.
The focus in version control is often code, but changes to analysis outputs should, when possible, be version controlled together with the code edits.
This way you know which edits in the code led to which changes in the outputs.
If you are writing code in Git or GitHub,
you can output plain text files such as \texttt{.tex} tables
and metadata saved in \texttt{.txt} or \texttt{.csv} to that directory.
Binary files that compile the tables,
as well as the complete datasets, on the other hand,
should be stored in your team's shared folder.
Whenever data cleaning or data construction codes are edited,
use the master script to run all the code for your project.
Git will highlight the changes that were in datasets and results that they entail.

%------------------------------------------------

\section{De-identifying research data}

The starting point for all tasks described in this chapter is the raw dataset,
which should contain the exact data received, with no changes or additions.
The raw data will invariably come in a variety of file formats and these files
should be saved in the raw data folder \textit{exactly as they were
received}. Be mindful of how and where they are stored as they cannot be
re-created and nearly always contain confidential data such as
\textbf{personally-identifying information}\index{personally-identifying information}.
As described in the previous chapter, confidential data must always be
encrypted\sidenote{\url{https://dimewiki.worldbank.org/Encryption}} and be
properly backed up since every other data file you will use is created from the
raw data. The only datasets that can not be re-created are the raw data
themselves.

The raw data files should never be edited directly. This is true even in the
rare case when the raw data cannot be opened due to, for example, incorrect
encoding where a non-English character is causing rows or columns to break at the
wrong place when the data is imported. In this scenario, you should create a
copy of the raw data where you manually remove the special characters and
securely back up \textit{both} the broken and the fixed copy of the raw data.
You will only keep working from the fixed copy, but you keep both copies in
case you later realize that the manual fix was done incorrectly.

The first step in the transformation of raw data to an analysis-ready dataset is de-identification.
This means stripping the dataset of personally identifying information.\sidenote{
	\url{https://dimewiki.worldbank.org/De-identification}}
This simplifies workflows, as once you create a de-identified version of the dataset,
you no longer need to interact directly with the encrypted raw data.
To do so, you will need to identify all variables that contain
identifying information.\sidenote{\url{
		https://www.povertyactionlab.org/sites/default/files/resources/J-PAL-guide-to-deidentifying-data.pdf}}
For data collection, where the research team designs the survey instrument,
flagging all potentially identifying variables in the questionnaire design stage
simplifies the initial de-identification process.
If you did not do that, or you received original data by another means,
there are a few tools to help flag variables with personally-identifying data.
JPAL's \texttt{PII scan}, as indicated by its name,
scans variable names and labels for common string patterns associated with identifying information.\sidenote{
	\url{https://github.com/J-PAL/PII-Scan}}
The World Bank's \texttt{sdcMicro}
lists variables that uniquely identify observations,
as well as allowing for more sophisticated disclosure risk calculations.\sidenote{
	\url{https://sdctools.github.io/sdcMicro/articles/sdcMicro.html}}
The \texttt{iefieldkit} command \texttt{iecodebook}
lists all variables in a dataset and exports an Excel sheet
where you can easily select which variables to keep or drop.\sidenote{
	\url{https://dimewiki.worldbank.org/Iecodebook}}

Once you have a list of variables that contain confidential information,
assess them against the analysis plan and first ask yourself for each variable:
\textit{will this variable be needed for the analysis?}
If not, the variable should be dropped.
Don't be afraid to drop too many variables the first time,
as you can always go back and remove variables from the list of variables to be dropped,
but you can not go back in time and drop a PII variable that was leaked
because it was incorrectly kept.
Examples include respondent names and phone numbers, enumerator names, taxpayer 
numbers, and addresses.
For each confidential variable that is needed in the analysis, ask yourself:
\textit{can I encode or otherwise construct a variable that masks the confidential component, and
then drop this variable?}
This is typically the case for most identifying information.
Examples include geocoordinates
(after constructing measures of distance or area,
drop the specific location)
and names for social network analysis (can be encoded to secret and unique IDs).
If the answer to either of the two questions above is yes,
all you need to do is write a script to drop the variables that are not required for analysis,
encode or otherwise mask those that are required,
and save a working version of the data.
If confidential information strictly required for the analysis itself and can not be
masked or encoded,
it will be necessary to keep at least a subset of the data encrypted through
the data analysis process.

The resulting de-identified data will be the underlying source for all cleaned and constructed data.
This is the dataset that you will interact with directly during the remaining tasks described in this chapter.
Because identifying information is typically only used during data collection,
when teams need to find and confirm the identity of interviewees,
de-identification should not affect the usability of the data.

\section{Cleaning data for analysis}

Data cleaning is the second stage in the transformation of raw data into data that you can analyze.
The cleaning process involves (1) making the dataset easy to use and understand,
and (2) documenting individual data points and patterns that may bias the analysis.
The underlying data structure does not change.
The cleaned dataset should contain only the variables collected in the field.
No modifications to data points are made at this stage, except for corrections of mistaken entries.

Cleaning is probably the most time-consuming of the stages discussed in this chapter.
You need to acquire an extensive understanding of the contents and structure of the raw data.
Explore the dataset using tabulations, summaries, and descriptive plots.
Knowing your dataset well will make it possible to do analysis.

\subsection{Identifying the identifier}

The first step in the cleaning process is to understand the level of observation in the data (what makes a row),
and what variable or set of variables uniquely identifies each observations.
Ensuring that observations are uniquely and fully identified\sidenote{\url{https://dimewiki.worldbank.org/ID\_Variable\_Properties}}
is possibly the most important step in data cleaning.
It may be the case that the variable expected to be the unique identifier in fact is either incomplete or contains duplicates.
This could be due to duplicate observations or errors in data entry.
It could also be the case that there is no identifying variable, or the identifier is a long string, such as a name.
In this case cleaning begins by carefully creating a variable that uniquely identifies the data.
As discussed in the previous chapter,
checking for duplicated entries is usually part of data quality monitoring,
and is ideally addressed as soon as data is received

Note that while modern survey tools create unique identifiers for each submitted data record,
that is not the same as having a unique ID variable for each individual in the sample.
You want to make sure the dataset has a unique ID variable
that can be cross-referenced with other records, such as the master dataset\sidenote{\url{https://dimewiki.worldbank.org/Master\_Data\_Set}}
and other rounds of data collection.
\texttt{ieduplicates} and \texttt{iecompdup},
two Stata commands included in the \texttt{iefieldkit}
package\index{iefieldkit},\sidenote{\url{https://dimewiki.worldbank.org/iefieldkit}}
create an automated workflow to identify, correct and document
occurrences of duplicate entries.

\subsection{Labeling, annotating, and finalizing clean data}

The last step of data cleaning is to label and annotate the data,
so that all users have the information needed to interact with it.
There are three key steps: renaming, labeling and recoding.
This is a key step to making the data easy to use, but it can be quite repetitive.
The \texttt{iecodebook} command suite, also part of \texttt{iefieldkit},
is designed to make some of the most tedious components of this process easier.\sidenote{
	\url{https://dimewiki.worldbank.org/iecodebook}}
\index{iecodebook}

First, \textbf{renaming}: for data with an accompanying survey instrument,
it is useful to keep the same variable names in the cleaned dataset as in the survey instrument.
That way it's straightforward to link variables to the relevant survey question.
Second, \textbf{labeling}: applying labels makes it easier to understand your data as you explore it,
and thus reduces the risk of small errors making their way through into the analysis stage.
Variable and value labels should be accurate and concise.\sidenote{
	\url{https://dimewiki.worldbank.org/Data\_Cleaning\#Applying\_Labels}}

Third, \textbf{recoding}: survey codes for ``Don't know'', ``Refused to answer'', and
other non-responses must be removed but records of them should still be kept. In Stata that can elegantly be done using extended missing values.\sidenote{
	\url{https://dimewiki.worldbank.org/Data\_Cleaning\#Survey\_Codes\_and\_Missing\_Values}}
String variables that correspond to categorical variables should be encoded.
Open-ended responses stored as strings usually have a high risk of being identifiers,
so they should be encoded into categories as much as possible and raw data points dropped.
You can use the encrypted data as an input to a construction script
that categorizes these responses and merges them to the rest of the dataset.

\subsection{Preparing a clean dataset}
The main output of data cleaning is the cleaned dataset.
It should contain the same information as the raw dataset,
with identifying variables and data entry mistakes removed.
Although original data typically requires more extensive data cleaning than secondary data,
you should carefully explore possible issues in any data you are about to use.
When reviewing raw data, you will inevitably encounter data entry mistakes,
such as typos and inconsistent values.
These mistakes should be fixed in the cleaned dataset,
and you should keep a careful record of how they were identified,
and how the correct value was obtained.\sidenote{\url{
		https://dimewiki.worldbank.org/Data\_Cleaning}}

The cleaned dataset should always be accompanied by a dictionary or codebook.
Survey data should be easily traced back to the survey instrument.
Typically, one cleaned dataset will be created for each data source
or survey instrument; and each row in the cleaned dataset represents one
respondent or unit of observation.\cite{tidy-data}

If the raw dataset is very large, or the survey instrument is very complex,
you may want to break the data cleaning into sub-steps,
and create intermediate cleaned datasets
(for example, one per survey module).
When dealing with complex surveys with multiple nested groups,
it is also useful to have each cleaned dataset at the smallest unit of observation inside a roster.
This will make the cleaning faster and the data easier to handle during construction.
But having a single cleaned dataset will help you with sharing and publishing the data.

Finally, any additional information collected only for quality monitoring purposes,
such as notes and duration fields, can also be dropped.
To make sure the cleaned dataset file doesn't get too big to be handled,
use commands such as \texttt{compress} in Stata to make sure the data
is always stored in the most efficient format.

Once you have a cleaned, de-identified dataset and the documentation to support it,
you have created the first data output of your project: a publishable dataset.
The next chapter will get into the details of data publication.
For now, all you need to know is that your team should consider submitting this dataset for publication,
even if it will remain embargoed for some time.
This will help you organize your files and create a backup of the data,
and some donors require that the data be filed as an intermediate step of the project.


\subsection{Documenting data cleaning}
Throughout the data cleaning process,
you will often need extensive inputs from the people responsible for data collection.
(This could be a survey team, the government ministry responsible for administrative data systems,
the technology firm that generated remote sensing data, etc.)
You should acquire and organize all documentation of how the data was generated, such as
reports from the data provider, field protocols, data collection manuals, survey instruments,
supervisor notes, and data quality monitoring reports.
These materials are essential for data documentation.\sidenote{
	\url{https://dimewiki.worldbank.org/Data\_Documentation}}
\index{Documentation}
They should be stored in the corresponding \texttt{Documentation} folder for easy access,
as you will probably need them during analysis,
and should be published along with the data.

Include in the \texttt{Documentation} folder records of any
corrections made to the data, including to duplicated entries,
as well as communications where theses issues are reported.
Be very careful not to include confidential information in documentation that is not securely stored,
or that you intend to release as part of a replication package or data publication.

Another important component of data cleaning documentation are the results of data exploration.
As you clean your dataset, take the time to explore the variables in it.
Use tabulations, summary statistics, histograms and density plots to understand the structure of data,
and look for potentially problematic patterns such as outliers,
missing values and distributions that may be caused by data entry errors.
Create a record of what you observe,
then use it as a basis for discussions of how to address data issues during variable construction.
This material will also be valuable during exploratory data analysis.

\section{Constructing analysis datasets}

% What is construction -------------------------------------
The third stage is construction of the dataset you will use for analysis.
It is at this stage that the cleaned data is transformed into analysis-ready data, 
by integrating different datasets and creating derived variables 
(dummies, indices, and interactions, to name a few),
as planned during research design\index{Research design},
and using the pre-analysis plan as a guide.\index{Pre-analysis plan}
During this process, the data points will typically be reshaped and aggregated
so that level of the dataset goes from the unit of observation in the survey 
to the unit of analysis.\sidenote{\url{
https://dimewiki.worldbank.org/Unit\_of\_Observation}}


A constructed dataset is built to answer an analysis question.
Since different pieces of analysis may require different samples,
or even different units of observation,
you may have one or multiple constructed datasets,
depending on how your analysis is structured.
Don't worry if you cannot create a single, ``canonical'' analysis dataset.
It is common to have many purpose-built analysis datasets.
Think of an agricultural intervention that was randomized across villages
and only affected certain plots within each village.
The research team may want to run household-level regressions on income,
test for plot-level productivity gains,
and check if village characteristics are balanced.
Having three separate datasets for each of these three pieces of analysis
will result in much cleaner do-files than if they all started from the same dataset.

\subsection{Fitting construction into the data workflow}
Construction is done separately from data cleaning for two reasons.
First, it clearly differentiates correction of data entry errors
(necessary for all interactions with the data)
from creation of analysis indicators (necessary only for the specific analysis).
Second, it ensures that variable definition is consistent across data sources.
Unlike cleaning, construction can create many outputs from many inputs.
Let's take the example of a project that has a baseline and an endline survey.
Unless the two instruments are exactly the same,
which is preferable but often not the case,
the data cleaning for them will require different steps,
and therefore will be done separately.
However, you still want the constructed variables to be calculated in the same way, so they are comparable.
To do this, you will require at least two cleaning scripts,
and a single one for construction.

Construction of the analysis data should be done right after data cleaning and before data analysis starts,
according to the pre-analysis plan.\index{Pre-analysis plan}
In practice, however, as you analyze the data,
different constructed variables may become necessary,
as well as subsets and other alterations to the data,
and you will need to adjust the analysis data accordingly.
Even if construction and analysis are done concurrently,
you should always do the two in separate scripts.
If every script that creates a table starts by loading a dataset,
subsetting it, and manipulating variables,
any edits to construction need to be replicated in all scripts.
This increases the chances that at least one of them will have a different sample or variable definition.
Doing all variable construction in a single, separate script helps
avoid this and ensure consistency across different outputs.

\subsection{Integrating different data sources}
Often, you will combine or merge information from different data sources together
in order to create the analysis dataset.
For example, you may merge administrative data with survey data
to include demographic information in your analysis,
or you may want to integrate geographic information
in order to construct indicators or controls based on the location of observations.
To do this, you will need to consider the unit of observation for each dataset,
and the identifying variable, to understand how they can be merged.

If the datasets you need to join have the same unit of observation,
merging may be straightforward.
The simplest case is merging datasets at the same unit of observation
which use a consistent, uniquely and fully identifying ID variable. 
For example, in the case of a panel survey for firms,
you may merge baseline and endline data using the firm identification number.
In many cases, however, 
datasets at the same unit of observation may not use a consistent numeric identifier.
Identifiers that are string variables, such as names, 
often contain spelling mistakes or irregularities in capitalization, spacing or ordering. 
In this case, you will need to do a \textbf{fuzzy match}, 
to link observations that have similar identifiers. 
In these cases, you will need to extensively analyze the merging patterns
and understand what units are present in one dataset but not the other,
as well as be able to resolve fuzzy or imperfect matching.
There are some commands such as \texttt{reclink} in Stata
that can provide some useful utilities,
but often a large amount of close examination is necessary
in order to figure out what the matching pattern should be
and how to accomplish it in practice through your code.

In other cases, you will need to join data sources that have different units of observation.
For example, you might be overlaying road location data with household data,
using a spatial match,
or combining school administrative data, such as attendance records and test scores,
with household demographic characteristics from a survey.
Sometimes these cases are conceptually straightforward.
For example, merging a dataset of health care providers
with a dataset of patients comes with a clear linking relation between the two;
the challenge usually occurs in correctly defining statistical aggregations
if the merge is intended to result in a dataset at the provider level.
However, other cases may not be designed with the intention to be merged together,
such as a dataset of infrastructure access points, for example, water pumps or schools
and a dataset of household locations and roads.
In those cases, a key part of the research contribution is figuring out what
a useful way to combine the datasets is.
Since these conceptual constructs are so important
and so easy to imagine different ways to do,
it is especially important that these data integrations are not treated mechanically
and are extensively documented separately from other data construction tasks.

Integrating different datasets may involve changing the structure of the data,
e.g. changing the unit of observation through collapses or reshapes.
This should always be done with great care. 
Two issues to pay extra attention to are missing values and dropped observations.
Merging, reshaping and aggregating data sets can change both the total number of observations
and the number of observations with missing values.
Make sure to read about how each command treats missing observations and,
whenever possible, add automated checks in the script that throw an error message if the result is different than what you expect.
If you are subsetting your data,
drop observations explicitly,
indicating why you are doing that and how the data set changed.

\subsection{Constructing analytical variables}
Once you have assembled your different data sources, 
it's time to create the specific indicators of interest for analysis. 
New variables should be assigned functional names, 
and the dataset ordered such that related variables are together.
Adding notes to each variable will make your dataset more user-friendly.


Before constructing new variables,
you must check and double-check the value-assignments of questions,
as well as the units and scales.
This is when you will use the knowledge of the data and the documentation you acquired during cleaning.
First, check that all categorical variables have the same value assignment, i.e.,
that labels and levels have the same correspondence across variables that use the same options.
For example, it's possible that in one question \texttt{0} means ``no'' and \texttt{1} means ``yes'',
while in another one the same answers were coded as \texttt{1} and \texttt{2}.
(We recommend coding binary questions as either \texttt{1} and \texttt{0} or \texttt{TRUE} and \texttt{FALSE},
so they can be used numerically as frequencies in means and as dummies in regressions.
Note that this implies re-expressing categorical variables like \texttt{sex} to binary variables like \texttt{woman}.)
Second, make sure that any numeric variables you are comparing are converted to the same scale or unit of measure.
You cannot add one hectare and two acres and get a meaningful number.

You will also need to decide how to handle any outliers or unusual values identified during data cleaning. 
How to treat outliers is a question for the research team (as there are multiple possible approaches),
but make sure to note what decision was made and why.
Results can be sensitive to the treatment of outliers,
so keeping the original variable in the dataset will allow you to test how much it affects the estimates.
These points also apply to imputation of missing values and other distributional patterns.

Finally, creating a panel with survey data involves additional timing complexities.
It is common to construct indicators soon after receiving data from a new survey round.
However, creating indicators for each round separately increases the risk of using different definitions every time.
Having a well-established definition for each constructed variable helps prevent that mistake,
but the best way to guarantee it won't happen is to create the indicators for all rounds in the same script.
Say you constructed variables after baseline, and are now receiving midline data.
Then the first thing you should do is create a cleaned panel dataset,
ignoring the previous constructed version of the baseline data.
The \texttt{iecodebook append} subcommand will help you reconcile and append the cleaned survey rounds.
After that, adapt a single variable construction script so it can be used on the panel dataset as a whole.
In addition to preventing inconsistencies,
this process will also save you time and give you an opportunity to review your original code.


\subsection{Documenting variable construction}

Because data construction involves translating concrete data points to more abstract measurements,
it is important to document exactly how each variable is derived or calculated.
Adding comments to the code explaining what you are doing and why is a crucial step both to prevent mistakes and to guarantee transparency.
To make sure that these comments can be more easily navigated,
it is wise to start writing a variable dictionary as soon as you begin making changes to the data.
Carefully record how specific variables have been combined, recoded, and scaled,
and refer to those records in the code.
This can be part of a wider discussion with your team about creating protocols for variable definition,
which will guarantee that indicators are defined consistently across projects.
When all your final variables have been created,
you can use the \texttt{iecodebook export} subcommand to list all variables in the dataset,
and complement it with the variable definitions you wrote during construction to create a concise metadata document.
Documentation is an output of construction as relevant as the code and the data.
Someone unfamiliar with the project should be able to understand the contents of the analysis datasets,
the steps taken to create them,
and the decision-making process through your documentation.
The construction documentation will complement the reports and notes created during data cleaning.
Together, they will form a detailed account of the data processing.

%------------------------------------------------

\section{Writing data analysis code}

% Intro --------------------------------------------------------------
When data is cleaned and indicators constructed, you are ready to generate analytical outputs.
\index{data analysis}
There are many existing resources for data analysis, such as
\textit{R for Data Science};\sidenote{\url{https://r4ds.had.co.nz}}
\textit{A Practical Introduction to Stata};\sidenote{\url{https://scholar.harvard.edu/files/mcgovern/files/practical\_introduction\_to\_stata.pdf}}
\textit{Mostly Harmless Econometrics};\sidenote{\url{https://www.researchgate.net/publication/51992844\_Mostly\_Harmless\_Econometrics\_An\_Empiricist's\_Companion}}
and \textit{Causal Inference: The Mixtape}.\sidenote{\url{https://scunning.com/mixtape.html}}
We focus on how to \textit{code} data analysis, rather than how to conduct specific analyses.

\subsection{Organizing analysis code}

The analysis stage usually starts with a process we call exploratory data analysis.
This is when you are trying different things and looking for patterns in your data.
It progresses into final analysis when your team starts to decide what are the main results,
those that will make it into the research output.
The way you deal with code and outputs for exploratory and final analysis is different.
During exploratory data analysis,
you will be tempted to write lots of analysis into one big, impressive, start-to-finish script.
It subtly encourages poor practices such as not clearing the workspace and not reloading the constructed dataset before each analysis task.
To avoid mistakes, it's important to take the time
to organize the code that you want to use again in a clean manner.

A well-organized analysis script starts with a completely fresh workspace
and explicitly loads data before analyzing it, for each output it creates.
This setup encourages data manipulation to be done earlier in the workflow
(that is, during construction).
It also and prevents you from accidentally writing pieces of analysis code that depend on one another
and require manual instructions for all necessary chunks of code to be run in the right order.
Each chunk of analysis code should run completely independently of all other code,
except for the master script.
You could go as far as coding every output in a separate script (although you usually won't).

There is nothing wrong with code files being short and simple.
In fact, analysis scripts should be as simple as possible,
so whoever is reading them can focus on the econometrics, not the coding.
All research questions and statistical decisions should be very explicit in the code,
and should be very easy to detect from the way the code is written.
This includes clustering, sampling, and control variables, to name a few.
If you have multiple analysis datasets,
each of them should have a descriptive name about its sample and unit of observation.
As your team comes to a decision about model specification,
you can create globals or objects in the master script to use across scripts.
This is a good way to make sure specifications are consistent throughout the analysis.
Using pre-specified globals or objects also makes your code more dynamic,
so it is easy to update specifications and results without changing every script.
It is completely acceptable to have folders for each task,
and compartmentalize each analysis as much as needed.

To accomplish this, you will need to make sure that you have an effective data management system,
including naming, file organization, and version control.
Just like you did with each of the analysis datasets,
name each of the individual analysis files descriptively.
Code files such as \path{spatial-diff-in-diff.do},
\path{matching-villages.R}, and \path{summary-statistics.py}
are clear indicators of what each file is doing, and allow you to find code quickly.
If you intend to numerically order the code as they appear in a paper or report,
leave this to near publication time.

\subsection{Visualizing data}

\textbf{Data visualization}\sidenote{\url{https://dimewiki.worldbank.org/Data\_visualization}} \index{data visualization}
is increasingly popular, and is becoming a field in its own right.\cite{healy2018data,wilke2019fundamentals}
Whole books have been written on how to create good data visualizations,
so we will not attempt to give you advice on it.
Rather, here are a few resources we have found useful.
The Tapestry conference focuses on ``storytelling with data''.\sidenote{
	\url{https://www.youtube.com/playlist?list=PLb0GkPPcZCVE9EAm9qhlg5eXMgLrrfMRq}}
\textit{Fundamentals of Data Visualization} provides extensive details on practical application;\sidenote{
	\url{https://serialmentor.com/dataviz}}
as does \textit{Data Visualization: A Practical Introduction}.\sidenote{
	\url{http://socvis.co}}
Graphics tools like Stata are highly customizable.
There is a fair amount of learning curve associated with extremely-fine-grained adjustment,
but it is well worth reviewing the graphics manual.\sidenote{\url{https://www.stata.com/manuals/g.pdf}}
For an easier way around it, Gray Kimbrough's \textit{Uncluttered Stata Graphs}
code is an excellent default replacement for Stata graphics that is easy to install.\sidenote{
  \url{https://graykimbrough.github.io/uncluttered-stata-graphs}}
If you are an R user, the \textit{R Graphics Cookbook}\sidenote{\url{https://r-graphics.org}}
is a great resource for the its popular visualization package \texttt{ggplot}\sidenote{
	\url{https://ggplot2.tidyverse.org}}.
But there are a variety of other visualization packages,
such as \texttt{highcharter},\sidenote{\url{http://jkunst.com/highcharter}}
\texttt{r2d3},\sidenote{\url{https://rstudio.github.io/r2d3}}
\texttt{leaflet},\sidenote{\url{https://rstudio.github.io/leaflet}}
and \texttt{plotly},\sidenote{\url{https://plot.ly/r}} to name a few.
We have no intention of creating an exhaustive list, but this is a good place to start.

We attribute some of the difficulty of creating good data visualization
to writing code to create them.
Making a visually compelling graph would already be hard enough if
you didn't have to go through many rounds of googling to understand a command.
The trickiest part of using plot commands is to get the data in the right format.
This is why we created the \textbf{Stata Visual Library}\sidenote{
	\url{https://worldbank.github.io/Stata-IE-Visual-Library}},
which has examples of graphs created in Stata and curated by us.\sidenote{A similar resource for R is \textit{The R Graph Gallery}. \\\url{https://www.r-graph-gallery.com}}
The Stata Visual Library includes example datasets to use with each do-file,
so you get a good sense of what your data should look like
before you can start writing code to create a visualization.

\subsection{Exporting analysis outputs}

Our team has created a few products to automate common outputs and save you
precious research time.
The \texttt{ietoolkit} package includes two commands to export nicely formatted tables.
\texttt{iebaltab}\sidenote{\url{https://dimewiki.worldbank.org/iebaltab}}
creates and exports balance tables to excel or {\LaTeX}.
\texttt{ieddtab}\sidenote{\url{https://dimewiki.worldbank.org/ieddtab}}
does the same for difference-in-differences regressions.
It also includes a command, \texttt{iegraph},\sidenote{
	\url{https://dimewiki.worldbank.org/iegraph}}
to export pre-formatted impact evaluation results graphs.

It's okay to not export each and every table and graph created during exploratory analysis.
Final analysis scripts, on the other hand, should export final outputs,
which are ready to be included to a paper or report.
No manual edits, including formatting, should be necessary after exporting final outputs --
those that require copying and pasting edited outputs,
in particular, are absolutely not advisable.
Manual edits are difficult to replicate,
and you will inevitably need to make changes to the outputs.
Automating them will save you time by the end of the process.
However, don't spend too much time formatting tables and graphs until you are ready to publish.\sidenote{
	For a more detailed discussion on this, including different ways to export tables from Stata, see \url{https://github.com/bbdaniels/stata-tables}}
Polishing final outputs can be a time-consuming process,
and you want to do it as few times as possible.

We cannot stress this enough:
don't ever set up a workflow that requires copying and pasting results.
Copying results from Excel to Word is error-prone and inefficient.
Copying results from a software console is risk-prone,
even more inefficient, and totally unnecessary.
There are numerous commands to export outputs from both R and Stata to a myriad of formats.\sidenote{
  Some examples are \url{http://repec.sowi.unibe.ch/stata/estout} ({\texttt{estout}}), \url{https://www.princeton.edu/~otorres/Outreg2.pdf} ({\texttt{outreg2}}),
and \url{https://www.benjaminbdaniels.com/stata-code/outwrite} ({\texttt{outwrite}}) in Stata,
and \url{https://cran.r-project.org/web/packages/stargazer/vignettes/stargazer.pdf} ({\texttt{stargazer}})
and \url{https://ggplot2.tidyverse.org/reference/ggsave.html} ({\texttt{ggsave}}) in R.}
Save outputs in accessible and, whenever possible, lightweight formats.
Accessible means that it's easy for other people to open them.
In Stata, that would mean always using \texttt{graph export} to save images as
\texttt{.jpg}, \texttt{.png}, \texttt{.pdf}, etc.,
instead of \texttt{graph save},
which creates a \texttt{.gph} file that can only be opened through a Stata installation.
Some publications require ``lossless'' TIFF or EPS files, which are created by specifying the desired extension.
Whichever format you decide to use, remember to always specify the file extension explicitly.
For tables there are less options and more consideration to be made.
Exporting table to \texttt{.tex} should be preferred.
Excel \texttt{.xlsx} and \texttt{.csv} are also commonly used,
but require the extra step of copying the tables into the final output.
The amount of work needed in a copy-paste workflow increases
rapidly with the number of tables and figures included in a research output,
and so do the chances of having the wrong version a result in your paper or report.

If you need to create a table with a very particular format
that is not automated by any command you know, consider writing it manually
(Stata's \texttt{filewrite}, for example, allows you to do that).
This will allow you to write a cleaner script that focuses on the econometrics,
and not on complicated commands to create and append intermediate matrices.
To avoid cluttering your scripts with formatting and ensure that formatting is consistent across outputs,
define formatting options in an R object or a Stata global and call them when needed.

Keep in mind that final outputs should be self-standing.
This means it should be easy to read and understand them with only the information they contain.
Make sure labels and notes cover all relevant information, such as sample,
unit of observation, unit of measurement and variable definition.\sidenote{
	\url{https://dimewiki.worldbank.org/Checklist:\_Reviewing\_Graphs} and
	\url{https://dimewiki.worldbank.org/Checklist:\_Submit\_Table}}

If you follow the steps outlined in this chapter,
most of the data work involved in the last step of the research process
-- publication -- will already be done.
If you used de-identified data for analysis,
publishing the cleaned dataset in a trusted repository will allow you to cite your data.
Some of the documentation produced during cleaning and construction can be published
even if the data cannot due to confidentiality.
Your analysis code will be organized in a reproducible way,
so all you will need to do when releasing a replication package is a last round of code review.
This will allow you to focus on what matters:
writing up your results into a compelling story.

%------------------------------------------------
