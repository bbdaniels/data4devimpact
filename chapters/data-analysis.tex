%------------------------------------------------

\begin{fullwidth}
Data analysis is hard. 
Transforming raw data into a substantial contribution to scientific knowledge 
requires a mix of subject expertise, programming skills, 
and statistical and econometric knowledge. 
The process of data analysis is, therefore, 
a back-and-forth discussion between people 
with differing experiences, perspectives, and research interests. 
The research assistant usually ends up being the pivot of this discussion. 
It is their job to translate the data received from the field into
economically meaningful indicators and to analyze them 
while making sure that code and outputs do not become tangled and lost over time.

When it comes to code, though, analysis is the easy part, 
as long as you have organized your data well. 
Of course, the econometrics behind data analysis is complex, 
but this is not a book on econometrics. 
Instead, this chapter will focus on how to organize your data work.
Most of a Research Assistant's time is spent cleaning data and getting it into the right format. 
When the practices recommended here are adopted,
it becomes much easier to analyze the data 
using commands that are already implemented in any statistical software. 


\end{fullwidth}

%------------------------------------------------

\section{Data management}
The goal of data management is to organize the components of data work 
so it can traced back and revised without massive effort.
In our experience, there are four key elements to good data management: 
folder structure, task breakdown, master scripts, and version control. 
A good folder structure organizes files so that any material can be found when needed.
It reflects a task breakdown into steps with well-defined inputs, tasks, and outputs.
This breakdown is applied to code, data sets, and outputs.
A master script connects folder structure and code.
It is a one-file summary of your whole project.
Finally, version histories and backups enable the team 
to edit files without fear of losing information.
Smart use of version control also allows you to track 
how each edit affects other files in the project.

% Task breakdown
We divide the process of turning raw data into analysis data in three stages: 
data cleaning, variable construction, and data analysis. 
Though they are frequently implemented at the same time, 
we find that creating separate scripts and data sets prevents mistakes. 
It will be easier to understand this division as we discuss what each stage comprises. 
What you should know by now is that each of these stages has well-defined inputs and outputs. 
This makes it easier to track tasks across scripts, 
and avoids duplication of code that could lead to inconsistent results. 
For each stage, there should be a code folder and a corresponding data set. 
The names of codes, data sets and outputs for each stage should be consistent,
making clear how they relate to one another. 
So, for example, a script called \texttt{clean-section-1} would create
a data set called \texttt{cleaned-section-1}.

% Folder structure
There are many schemes to organize research data. 
Our preferred scheme reflects this task breakdown.
\index{data organization}
We created the \texttt{iefolder}\sidenote{
	\url{https://dimewiki.worldbank.org/wiki/iefolder}}
package (part of \texttt{ietoolkit}\sidenote{
	\url{https://dimewiki.worldbank.org/wiki/ietoolkit}})
based on our experience with primary survey data,
but it can be used for different types of data. 
\texttt{iefolder} is designed to standardize folder structures across teams and projects.
This means that PIs and RAs face very small costs when switching between projects, 
because they are organized in the same way. 
\sidenote{\url{https://dimewiki.worldbank.org/wiki/DataWork_Folder}}
At the first level of this folder are what we call survey round folders. 
\sidenote{\url{https://dimewiki.worldbank.org/wiki/DataWork_Survey_Round}}
You can think of a round as one source of data, 
that will be cleaned in the same manner. 
Inside round folders, there are dedicated folders for 
raw (encrypted) data; de-identified data; cleaned data; and final (constructed) data. 
There is a folder for raw results, as well as for final outputs. 
The folders that hold code are organized in parallel to these, 
so that the progression through the whole project can be followed by anyone new to the team.  
Additionally, \texttt{iefolder} creates \textbf{master do-files}
\sidenote{\url{https://dimewiki.worldbank.org/wiki/Master_Do-files}} 
so all project code is reflected in a top-level script.

% Master do file
Master scripts allow users to execute all the project code from a single file.
It briefly describes what each code, 
and maps the files they require and create. 
It also connects code and folder structure through globals or objects. 
In short, a master script is a human-readable map to the tasks, 
files and folder structure that comprise a project.  
Having a master script eliminates the need for complex instructions to replicate results. 
Reading the master do-file should be enough for anyone who's unfamiliar with the project
to understand what are the main tasks, which scripts execute them,
and where different files can be found in the project folder. 
That is, it should contain all the information needed to interact with a project's data work.

% Version control
Finally, everything that can be version-controlled should be. 
Version control allows you to effectively track code edits,
including the addition and deletion of files. 
This way you can delete code you no longer need, 
and still recover it easily if you ever need to get back previous work.
Both analysis results and data sets will change with the code.
You should have each of them stored with the code that created it.
If you are writing code in Git/GitHub,
you can output plain text files such as \texttt{.tex} tables,
and meta data saved in \texttt{.txt} or \texttt{.csv} to that directory.
Binary files that compile the tables,
as well as the complete data sets, on the other hand,
should be stored in your team's shared folder. 
Whenever data cleaning or data construction codes are edited,
use the master script to run all the code for your project.
Git will highlight the changes that were in data sets and results that they entail. 

%------------------------------------------------

\section{Data cleaning}

% intro: what is data cleaning -------------------------------------------------
Data cleaning is the first stage of transforming the data you received from the field into data that you can analyze.
\sidenote{\url{https://dimewiki.worldbank.org/wiki/Data_Cleaning}}
The cleaning process involves (1) making the data set easily usable and understandable, and (2) documenting individual data points and patterns that may bias the analysis.
The underlying data structure does not change.
The cleaned data set should contain only the data collected in the field.
No modifications to data points are made at this stage, except for corrections of mistaken entries.

Cleaning is probably the most time consuming of the stages discussed in this chapter.
This is the time when you obtain an extensive understanding of  the contents and structure of the data that was collected.
Explore your data set using tabulations, summaries, and descriptive plots.
You should use this time to understand the types of responses collected, both within each survey question and across respondents.
Knowing your data set well will make it possible to do analysis.


% Deidentification ------------------------------------------------------------------
The initial input for data cleaning is the raw data.
It should contain only materials that are received directly from the field.
They will invariably come in a host of file formats and nearly always contain personally-identifying information.\index{personally-identifying information}
These files should be retained in the raw data folder \textit{exactly as they were received}.
The folder must be encrypted if it is shared in an insecure fashion,
\sidenote{\url{https://dimewiki.worldbank.org/wiki/Encryption}}.
and it must be backed up in a secure offsite location.
Everything else can be replaced, but raw data cannot.
Therefore, raw data should never be interacted with directly.

Secure storage of the raw
\sidenote{\url{https://dimewiki.worldbank.org/wiki/Data_Security}}
data means access to it will be restricted even inside the research team.
Loading encrypted data multiple times it can be annoying.
To facilitate the handling of the data, remove any personally identifiable information from the data set.
This will create a de-identified data set, that can be saved in a non-encrypted folder. 
De-identification,
\sidenote{\url{https://dimewiki.worldbank.org/wiki/De-identification}}
at this stage, means stripping the data set of direct identifiers such as names, phone numbers, addresses, and geolocations.
\sidenote{\url{ https://www.povertyactionlab.org/sites/default/files/resources/J-PAL-guide-to-deidentifying-data.pdf }}
The resulting de-identified data will be the underlying source for all cleaned and constructed data.
Because identifying information is typically only used during data collection, 
to find and confirm the identity of interviewees, 
de-identification should not affect the usability of the data.
In fact, most identifying information can be converted into non-identified variables for analysis purposes
(e.g. GPS coordinates can be translated into distances). 
However, if sensitive information is strictly needed for analysis,
all the tasks described in this chapter must be performed in a secure environment.

% Unique ID and data entry corrections ---------------------------------------------
There are two main cases when the raw data will be modified during data cleaning.
The first one is when there are duplicated entries in the data.
Ensuring that observations are uniquely and fully identified\sidenote{\url{https://dimewiki.worldbank.org/wiki/ID_Variable_Properties}}
is possibly the most important step in data cleaning
(as anyone who ever tried to merge data sets that are not uniquely identified knows).
Modern survey tools create unique observation identifiers.
That, however, is not the same as having a unique ID variable for each individual in the sample.
You want to make sure the data set has a unique ID variable
that can be cross-referenced with other records, such as the Master Data Set\sidenote{\url{https://dimewiki.worldbank.org/wiki/Master_Data_Set}}
and other rounds of data collection.
\texttt{ieduplicates} and \texttt{iecompdup}, 
two commands  included in the \texttt{ietoolkit} package,
create an automated workflow to identify, correct and document
occurrences of duplicate entries. 

Looking for duplicated entries is usually part of data quality monitoring,
as is the only other reason to change the raw data during cleaning:
correcting mistakes in data entry.
During data quality monitoring, you will inevitably encounter data entry mistakes,
such as typos and inconsistent values.
If you don't, you are probably not doing a very good job at looking for them.
These mistakes should be fixed in the cleaned data set,
and you should keep a careful record of how they were identified,
and how the correct value was obtained.

% Data description ------------------------------------------------------------------
Note that if you are using secondary data, 
the tasks described above can likely be skipped.
However, the last step of data cleaning, describing the data,
will probably still be necessary.
This is a key step to making the data easy to use, but it can be quite repetitive.
The \texttt{iecodebook} command suite, part of \texttt{iefieldkit},
is designed to make some of the most tedious components of this process,
such as renaming, relabeling, and value labeling,
much easier (including in data appending).\sidenote{\url{https://dimewiki.worldbank.org/wiki/iecodebook}}
\index{iecodebook}
We have a few recommendations on how to use this command for data cleaning.
First, we suggest keeping the same variable names as in the survey instrument,
so it's easy to connect the two files.
Don't skip the labelling!
Applying labels makes it easier to understand what the data is showing while exploring the data. 
This minimizes the risk of small errors making their way through into the analysis stage.
Variable and value labels should be accurate and concise.\sidenote{\url{https://dimewiki.worldbank.org/wiki/Data\_Cleaning\#Applying\_Labels}}
Recodes should be used to turn codes for "Don't know", "Refused to answer", and
other non-responses into extended missing values.\sidenote{\url{https://dimewiki.worldbank.org/wiki/Data\_Cleaning\#Survey\_Codes\_and\_Missing\_Values}}
String variables need to be encoded, and open-ended responses, categorized or dropped\sidenote{\url{https://dimewiki.worldbank.org/wiki/Data\_Cleaning\#Strings}}
(unless you are using qualitative or classification analyses, which are less common).
Finally, any additional information collected only for quality monitoring purposes,
such as notes and duration field, can also be dropped.

% Outputs -----------------------------------------------------------------

% Data set
The most important output of data cleaning is the cleaned data set. 
It should contain the same information as the raw data set,
with no changes to data points.
It should also be easily traced back to the survey instrument,
and be accompanied by a dictionary or codebook.
Typically, one cleaned data set will be created for each data source.
Each row in the cleaned data set represents one survey entry or unit of observation.
\sidenote{\cite{tidy-data}}
If the raw data set is very large, or the survey instrument is very complex,
you may want to break the data cleaning into sub-steps, 
and create intermediate cleaned data sets
(for example, one per survey module).
Breaking cleaned data sets into the smallest unit of observation inside a roster
make the cleaning faster and the data easier to handle during construction.
But having a single cleaned data set will help you with sharing and publishing the data.
To make sure this file doesn't get too big to be handled,
use commands such as \texttt{compress} in Stata to make sure the data
is always stored in the most efficient format.

% Documentation
Throughout the data cleaning process, you will need inputs from the field, 
including enumerator manuals, survey instruments, 
supervisor notes, and data quality monitoring reports.
These materials are part of what we call data documentation
\sidenote{\url{https://dimewiki.worldbank.org/wiki/Data_Documentation}}
\index{Documentation},
and should be stored in the corresponding folder, 
as you will probably need them during analysis and publication.
Include in the \texttt{Documentation} folder records of any
corrections made to the data, including to duplicated entries,
as well as communications from the field where theses issues are reported.
Make sure to also have a record of potentially problematic patterns you noticed
while exploring the data, such as outliers and variables with many missing values.
Be very careful not to include sensitive information in 
documentation that is not securely stored, 
or that you intend to release as part of a replication package or data publication.

\section{Indicator construction}

% What is construction -------------------------------------
Any changes to the original data set happen during construction.
It is at this stage that the raw data is transformed into analysis data.
This is done by creating derived variables
(binaries, indices, and interactions, to name a few).
To understand why construction is necessary,
let's take the example of a survey's consumption module.
It will result in separate variables indicating the 
amount of each item in the bundle that was consumed.
There may be variables indicating the cost of these items.
You cannot run a meaningful regression on these variables. 
You need to manipulate them into something that has \textit{economic} meaning. 

\textcolor{red}{During this process, the data points will typically be reshaped and aggregated 
so that level of the data set goes from the unit of observation in the survey to the unit of analysis. 
\sidenote{\url{https://dimewiki.worldbank.org/wiki/Unit_of_Observation}} 
In the consumption module example, the unit of observation is one good. 
The survey will probably also include a household composition module,
where age and gender variable will be defined for each household member.
The typical unit of analysis for such consumption indices, on the other hand, is the household.
So to calculate the final indicator, 
the number of household members will be aggregated to the household level;
then the expenditure or quantity consumed will be aggregated as well;
and finally the result of the latter aggregation will be divided by the result of the former.}

% Why it is a separate process -------------------------------

% From cleaning
Construction is done separately from data cleaning for two reasons. 
First, so you have a clear cut of what was the data originally received,
and what is the result of data processing decisions.
Second, because if you have different data sources, 
say a baseline and an endline survey, 
unless the two instruments were exactly the same, 
the data cleaning will differ between the two,
but you want to make sure that variable definition is consistent across sources. 
So you want to first merge the data sets and then create the variables only once.

% From analysis
Data construction is never a finished process.
It comes ``before'' data analysis only in a limited sense: the construction code must be run before the analysis code.
Typically, however, construction and analysis code are written concurrently.
As you write the analysis, different constructed variables will become necessary, as well as subsets and other alterations to the data.
Still, constructing variables in a separate script from the analysis will help you ensure consistency across different outputs. 
If every script that creates a table starts by loading a data set, subsetting it and manipulating variables, any edits to construction need to be replicated in all scripts. 
This increases the chances that at least one of them will have a different sample or variable definition.


% What to do during construction -----------------------------------------
Construction is the step where you face the largest risk of making a mistake that will affect your results. 
Keep in mind that details and scales matter. 
It is important to check and double-check the value-assignments of questions and their scales before constructing new variables using them. 
Are they in percentages or proportions? 
Are all variables you are combining into an index or average using the same scale? 
Are yes or no questions coded as 0 and 1, or 1 and 2?
This is when you will use the knowledge of the data you acquired and the documentation you created during the cleaning step the most.

Adding comments to the code explaining what you are doing and why is crucial here.
There are always ways for things to go wrong that you never anticipated, but two issues to pay extra attention to are missing values and dropped observations. 
If you are subsetting a data set, drop observations explicitly, indicating why you are doing that and how the data set changed.
Merging, reshaping and aggregating data sets can change both the total number of observations and the number of observations with missing values.
Make sure to read about how each command treats missing observations and, whenever possible, add automated checks in the script that throw an error message if the result is changing.

At this point, you will also need to address some of the issues in the data that you identified during data cleaning. 
The most common of them is the presence of outliers.
How to treat outliers is a research question, but make sure to note what we the decision made by the research team, and how you came to it. 
Results can be sensitive to the treatment of outliers, so keeping the original variable in the data set will allow you to test how much it affects the estimates.

% Outputs -----------------------------------------------------------------

% Data set
The outputs of construction are the data sets that will be used for analysis.
The level of observation of a constructed data set is the unit analysis. 
Each data set is purpose-built to answer an analysis question.
Since different pieces of analysis may require different samples,
or even different units of observation,
you may have one or multiple constructed data sets, 
depending on how your analysis is structured.
So don't worry if you cannot create a single, ``canonical'' analysis data set.
It is common to have many purpose-built analysis datasets:
there may be a \texttt{data-wide.dta},
\texttt{data-wide-children-only.dta}, \texttt{data-long.dta},
\texttt{data-long-counterfactual.dta}, and many more as needed.
One thing all constructed data sets should have in common, though,
are functionally-named variables.
As you no longer need to worry about keeping variable names
consistent with the survey, they should be as intuitive as possible.

% Documentation
It is wise to start an explanatory guide as soon as you start making changes to the data.
Carefully record how specific variables have been combined, recoded, and scaled. 
This can be part of a wider discussion with your team about creating protocols for variable definition.
That will guarantee that indicators are defined consistently across projects.
Documentation is an output of construction as relevant as the codes.
Someone unfamiliar with the project should be able to understand the contents of the analysis data sets, the steps taken to create them, and the decision-making process through your documentation.
The construction documentation will complement the reports and notes created during data cleaning.
Together, they will form a detailed account of the data processing.



%------------------------------------------------

\section{Writing data analysis code}

% Intro --------------------------------------------------------------
Data analysis is the stage of the process when research outputs are created. 
\index{data analysis}
Many introductions to common code skills and analytical frameworks exist, such as
\textit{R for Data Science};\sidenote{\url{https://r4ds.had.co.nz/}}
\textit{A Practical Introduction to Stata};\sidenote{\url{https://scholar.harvard.edu/files/mcgovern/files/practical_introduction_to_stata.pdf}}
\textit{Mostly Harmless Econometrics};\sidenote{\url{https://www.researchgate.net/publication/51992844_Mostly_Harmless_Econometrics_An_Empiricist's_Companion}} and
\textit{Causal Inference: The Mixtape}.\sidenote{\url{http://scunning.com/mixtape.html}}
This section will not include instructions on how to conduct specific analyses,
as these are highly specialized and require field and statistical expertise.
Instead, we will outline the structure of writing analysis code,
assuming you have completed the process of data cleaning and construction.

% Exploratory and final data analysis -----------------------------------------
Data analysis can be divided into two steps. 
The first, which we will call exploratory data analysis, 
is when you are trying different things and looking for patterns in your data. 
The second step, the final analysis,
happens when the research has matured,
and your team has decided what pieces of analysis will make into the research output.

% Organizing scripts ---------------------------------------------------------
During exploratory data analysis,
you will be tempted to write lots of analysis 
into one big, impressive, start-to-finish analysis file. 
Though it's fine to write such a script during a long analysis meeting, 
this practice is error-prone, 
because it subtly encourages poor practices such as 
not clearing the workspace or loading fresh data. 
It's important to take the time to organize scripts in a clean manner and to avoid mistakes.

A well-organized analysis script starts with 
a completely fresh workspace and load its data directly
prior to running that specific analysis.
This encourages data manipulation to be done upstream (that is, during construction),
and prevents you from accidentally writing pieces of code
that depend on each other, leading to the too-familiar
``run this part, then that part, then this part'' process.
Each script should run completely independently of all other code.
You can go as far as coding every output in a separate file.
There is nothing wrong with code files being short and simple --
as long as they directly correspond to specific pieces of analysis.

To accomplish this, you will need to make sure that you
have an effective system of naming, organization, and version control.
Just like you named each of the analysis datasets,
each of the individual analysis files should be descriptively named.
Code files such as \path{spatial-diff-in-diff.do},
\path{matching-villages.do}, and \path{summary-statistics.do}
are clear indicators of what each file is doing
and allow code to be found very quickly.

Analysis files should be as simple as possible, 
so you can focus on the econometrics.
The first thing any analysis code does is to load a data set
and explicitly set the analysis sample.
In fact, all research decisions, not only the sampling,
should be made very explicit in the code.
This includes clustering, sampling and use of control variables. 
As you come a decision on what are the main specifications to be reported,
you can create globals or objects in the master do file 
that set these options.
This is a good way to make sure specifications are consistent throughout the analysis, 
apart from being very dynamic and making it easy to update all scripts if needed.
It is completely acceptable to have folders for each task,
and compartmentalize each analysis as much as needed.
It is always better to have more code files open
than to have to scroll around repeatedly inside a given file.

% Outputs -----------------------------------------------------

% Exploratory analysis
It's ok to not export each and every table and graph created during exploratory analysis. 
Instead, we suggest running them into markdown files using RMarkdown or 
the different dynamic document options available in Stata. 
This will allow you to update and present results quickly, 
while maintaining a record the different analysis explored. 
% Final analysis
Final analysis scripts, on the other hand, should export final outputs: 
these are ready to be included to a paper or report; and
no manual edits, including formatting, should be necessary after running them. 
Manual edits are difficult to replicate, 
and you will end up having to make changes to the outputs,
so automating them will save you time by the end of the process. 
Don't ever set a workflow that includes copying and pasting results printed in the console.
% Output content
Finally, a final outputs should be self-standing,
meaning they are easy to read and understand 
with only the information they contain.
To accomplish this, labels and notes should cover all 
relevant information such as
sample, unit of observation, unit of measurement and variable definition. 
\sidenote{\url{https://dimewiki.worldbank.org/wiki/Checklist:_Reviewing_Graphs}}
\sidenote{\url{https://dimewiki.worldbank.org/wiki/Checklist:_Submit_Table}}

% Output formats
Outputs should be saved in accessible and, whenever possible, lightweight formats.
% Figures
Accessible means that other people can easily open them  -- 
in Stata, that would mean always using \texttt{graph export} 
to save images as \texttt{.jpg}, \texttt{.png}, \texttt{.pdf}, etc.,
instead of \texttt{graph save}, which creates a \texttt{.gph} file 
that can only be opened through a Stata installation.
\texttt{.tif} and \texttt{.eps} are two examples of accessible lightweight formats,
and \texttt{.eps} has the added advantage of allowing a designer to edit the images for printing.
% Tables
For tables, \texttt{.tex} is preferred. 
A variety of packages in both R and Stata export tables in this format.
Excel \texttt{.xlsx} and \texttt{.csv} files are also acceptable,
although if you are working on a large report they will become cumbersome to update after revisions.

% Formatting
Don't spend too much time formatting tables and graphs until you are ready to publish. 
Polishing final outputs can be a time-consuming process, 
and you want to avoid doing it multiple times.
If you need to create a table with a very particular format, 
that is not automated by any command you know,
consider writing the table manually 
(Stata's \texttt{filewrite}, for example, allows you to do that).
This will allow you to write a cleaner script that focuses on the econometrics, 
and not on complicated commands to create and append intermediate matrices.
To avoid cluttering your scripts with formatting and ensure that formatting is consistent across outputs,
define formatting options in an R object or a Stata global and call them when needed.

%------------------------------------------------

\section{Visualizing data}

\textbf{Data visualization}\sidenote{\url{https://dimewiki.worldbank.org/wiki/Data_visualization}} is increasingly popular,
\index{data visualization}
but a great deal of it is very low quality.\cite{healy2018data,wilke2019fundamentals}
The default graphics settings in Stata, for example,
are pretty bad.\sidenote{Gray Kimbrough's
\textit{Uncluttered Stata Graphs} code is an excellent
default replacement that is easy to install.
\url{https://graykimbrough.github.io/uncluttered-stata-graphs/}}
Thankfully, graphics tools like Stata are highly customizable.
There is a fair amount of learning curve associated with
extremely-fine-grained adjustment,
but it is well worth reviewing the graphics manual
to understand how to apply basic colors, alignments, and labelling tools\sidenote{
\url{https://www.stata.com/manuals/g.pdf}}
The manual is worth skimming in full, as it provides
many visual examples and corresponding code examples
that detail how to produce them.

Graphics are hard to get right because you, as the analyst,
will already have a pretty good idea of what you are trying to convey.
Someone seeing the illustration for the first time,
by contrast, will have no idea what they are looking at.
Therefore, a visual image has to be compelling in the sense
that it shows that there is something interesting going on
and compels the reader to figure out exactly what it is.
A variety of resources can help you
figure out how best to structure a given visual.
The \textbf{Stata Visual Library}\sidenote{
\url{https://worldbank.github.io/Stata-IE-Visual-Library/}}
 has many examples of figures that communicate effectively.
The Tapestry conference focuses on ``storytelling with data''.\sidenote{
\url{https://www.youtube.com/playlist?list=PLb0GkPPcZCVE9EAm9qhlg5eXMgLrrfMRq}}
\textit{Fundamentals of Data Visualization} provides extensive details on practical application;\sidenote{
\url{https://serialmentor.com/dataviz}}
as does \textit{Data Visualization: A Practical Introduction}.\sidenote{
\url{http://socvis.co}}




%------------------------------------------------
