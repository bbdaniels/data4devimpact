%------------------------------------------------

\begin{fullwidth}
Data analysis is hard. 
Transforming raw data into a substantial contribution to scientific knowledge 
requires a mix of subject expertise, programming skills, 
and statistical and econometric knowledge. 
The process of data analysis is, therefore, 
a back-and-forth discussion between people 
with differing experiences, perspectives, and research interests. 
The research assistant usually ends up being the pivot of this discussion. 
It is their job to translate the data received from the field into
economically meaningful indicators and to analyze them 
while making sure that code and outputs do not become tangled and lost over time.

When it comes to code, though, analysis is the easy part, 
as long as you have organized your data well. 
Of course, the econometrics behind data analysis is complex, 
but this is not a book on econometrics. 
Instead, this chapter will focus on how to organize your data work.
Most of a Research Assistant's time is spent cleaning data and getting it into the right format. 
When the practices recommended here are adopted,
it becomes much easier to analyze the data 
using commands that are already implemented in any statistical software. 


\end{fullwidth}

%------------------------------------------------

\section{Data management}
The goal of data management is to organize the components of data work 
so it can traced back and revised without massive effort.
In our experience, there are four key elements to good data management: 
folder structure, task breakdown, master scripts, and version control. 
A good folder structure organizes files so that any material can be found when needed.
It reflects a task breakdown into steps with well-defined inputs, tasks, and outputs.
This breakdown is applied to code, data sets, and outputs.
A master script connects folder structure and code.
It is a one-file summary of your whole project.
Finally, version histories and backups enable the team 
to edit files without fear of losing information.
Smart use of version control also allows you to track 
how each edit affects other files in the project.

% Task breakdown
We divide the process of turning raw data into analysis data in three stages: 
data cleaning, variable construction, and data analysis. 
Though they are frequently implemented at the same time, 
we find that creating separate scripts and data sets prevents mistakes. 
It will be easier to understand this division as we discuss what each stage comprises. 
What you should know by now is that each of these stages has well-defined inputs and outputs. 
This makes it easier to track tasks across scripts, 
and avoids duplication of code that could lead to inconsistent results. 
For each stage, there should be a code folder and a corresponding data set. 
The names of codes, data sets and outputs for each stage should be consistent,
making clear how they relate to one another. 
So, for example, a script called \texttt{clean-section-1} would create
a data set called \texttt{cleaned-section-1}.

The division of a project in stages also helps the review workflow inside your team.
The code, data and outputs of each of these stages should go through at least one round of code review.
During the code review process, team members should read and run each other's codes.
Doing this at the end of each stage helps prevent the amount of work to be reviewed to become too overwhelming.
Code review is a common quality assurance practice among data scientists.
It helps to keep the level of the outputs high, and is also a great way to learn and improve your code.

% Folder structure
There are many schemes to organize research data. 
Our preferred scheme reflects the task breakdown just discussed.
\index{data organization}
We created the \texttt{iefolder}\sidenote{
	\url{https://dimewiki.worldbank.org/wiki/iefolder}}
package (part of \texttt{ietoolkit}\sidenote{
	\url{https://dimewiki.worldbank.org/wiki/ietoolkit}})
based on our experience with primary survey data,
but it can be used for different types of data. 
\texttt{iefolder} is designed to standardize folder structures across teams and projects.
This means that PIs and RAs face very small costs when switching between projects, 
because they are organized in the same way. 
\sidenote{\url{https://dimewiki.worldbank.org/wiki/DataWork_Folder}}
At the first level of this folder are what we call survey round folders. 
\sidenote{\url{https://dimewiki.worldbank.org/wiki/DataWork_Survey_Round}}
You can think of a round as one source of data, 
that will be cleaned in the same manner. 
Inside round folders, there are dedicated folders for 
raw (encrypted) data; de-identified data; cleaned data; and final (constructed) data. 
There is a folder for raw results, as well as for final outputs. 
The folders that hold code are organized in parallel to these, 
so that the progression through the whole project can be followed by anyone new to the team.  
Additionally, \texttt{iefolder} creates \textbf{master do-files}
\sidenote{\url{https://dimewiki.worldbank.org/wiki/Master_Do-files}} 
so all project code is reflected in a top-level script.

% Master do file
Master scripts allow users to execute all the project code from a single file.
It briefly describes what each code, 
and maps the files they require and create. 
It also connects code and folder structure through globals or objects. 
In short, a master script is a human-readable map to the tasks, 
files and folder structure that comprise a project.  
Having a master script eliminates the need for complex instructions to replicate results. 
Reading the master do-file should be enough for anyone who's unfamiliar with the project
to understand what are the main tasks, which scripts execute them,
and where different files can be found in the project folder. 
That is, it should contain all the information needed to interact with a project's data work.

% Version control
Finally, everything that can be version-controlled should be. 
Version control allows you to effectively track code edits,
including the addition and deletion of files. 
This way you can delete code you no longer need, 
and still recover it easily if you ever need to get back previous work.
Both analysis results and data sets will change with the code.
You should have each of them stored with the code that created it.
If you are writing code in Git/GitHub,
you can output plain text files such as \texttt{.tex} tables,
and meta data saved in \texttt{.txt} or \texttt{.csv} to that directory.
Binary files that compile the tables,
as well as the complete data sets, on the other hand,
should be stored in your team's shared folder. 
Whenever data cleaning or data construction codes are edited,
use the master script to run all the code for your project.
Git will highlight the changes that were in data sets and results that they entail. 

%------------------------------------------------

\section{Data cleaning}

% intro: what is data cleaning -------------------------------------------------
Data cleaning is the first stage of transforming the data you received from the field into data that you can analyze.
\sidenote{\url{https://dimewiki.worldbank.org/wiki/Data_Cleaning}}
The cleaning process involves (1) making the data set easily usable and understandable, and (2) documenting individual data points and patterns that may bias the analysis.
The underlying data structure does not change.
The cleaned data set should contain only the data collected in the field.
No modifications to data points are made at this stage, except for corrections of mistaken entries.

Cleaning is probably the most time consuming of the stages discussed in this chapter.
This is the time when you obtain an extensive understanding of  the contents and structure of the data that was collected.
Explore your data set using tabulations, summaries, and descriptive plots.
You should use this time to understand the types of responses collected, both within each survey question and across respondents.
Knowing your data set well will make it possible to do analysis.


% Deidentification ------------------------------------------------------------------
The initial input for data cleaning is the raw data.
It should contain only materials that are received directly from the field.
They will invariably come in a host of file formats and nearly always contain personally-identifying information.\index{personally-identifying information}
These files should be retained in the raw data folder \textit{exactly as they were received}.
The folder must be encrypted if it is shared in an insecure fashion,
\sidenote{\url{https://dimewiki.worldbank.org/wiki/Encryption}}.
and it must be backed up in a secure offsite location.
Everything else can be replaced, but raw data cannot.
Therefore, raw data should never be interacted with directly.

Secure storage of the raw
\sidenote{\url{https://dimewiki.worldbank.org/wiki/Data_Security}}
data means access to it will be restricted even inside the research team.
Loading encrypted data multiple times it can be annoying.
To facilitate the handling of the data, remove any personally identifiable information from the data set.
This will create a de-identified data set, that can be saved in a non-encrypted folder. 
De-identification,
\sidenote{\url{https://dimewiki.worldbank.org/wiki/De-identification}}
at this stage, means stripping the data set of direct identifiers such as names, phone numbers, addresses, and geolocations.
\sidenote{\url{ https://www.povertyactionlab.org/sites/default/files/resources/J-PAL-guide-to-deidentifying-data.pdf }}
The resulting de-identified data will be the underlying source for all cleaned and constructed data.
Because identifying information is typically only used during data collection, 
to find and confirm the identity of interviewees, 
de-identification should not affect the usability of the data.
In fact, most identifying information can be converted into non-identified variables for analysis purposes
(e.g. GPS coordinates can be translated into distances). 
However, if sensitive information is strictly needed for analysis,
all the tasks described in this chapter must be performed in a secure environment.

% Unique ID and data entry corrections ---------------------------------------------
There are two main cases when the raw data will be modified during data cleaning.
The first one is when there are duplicated entries in the data.
Ensuring that observations are uniquely and fully identified\sidenote{\url{https://dimewiki.worldbank.org/wiki/ID_Variable_Properties}}
is possibly the most important step in data cleaning
(as anyone who ever tried to merge data sets that are not uniquely identified knows).
Modern survey tools create unique observation identifiers.
That, however, is not the same as having a unique ID variable for each individual in the sample.
You want to make sure the data set has a unique ID variable
that can be cross-referenced with other records, such as the Master Data Set\sidenote{\url{https://dimewiki.worldbank.org/wiki/Master_Data_Set}}
and other rounds of data collection.
\texttt{ieduplicates} and \texttt{iecompdup}, 
two commands  included in the \texttt{ietoolkit} package,
create an automated workflow to identify, correct and document
occurrences of duplicate entries. 

Looking for duplicated entries is usually part of data quality monitoring,
as is the only other reason to change the raw data during cleaning:
correcting mistakes in data entry.
During data quality monitoring, you will inevitably encounter data entry mistakes,
such as typos and inconsistent values.
If you don't, you are probably not doing a very good job at looking for them.
These mistakes should be fixed in the cleaned data set,
and you should keep a careful record of how they were identified,
and how the correct value was obtained.

% Data description ------------------------------------------------------------------
Note that if you are using secondary data, 
the tasks described above can likely be skipped.
However, the last step of data cleaning, describing the data,
will probably still be necessary.
This is a key step to making the data easy to use, but it can be quite repetitive.
The \texttt{iecodebook} command suite, part of \texttt{iefieldkit},
is designed to make some of the most tedious components of this process,
such as renaming, relabeling, and value labeling,
much easier (including in data appending).\sidenote{\url{https://dimewiki.worldbank.org/wiki/iecodebook}}
\index{iecodebook}
We have a few recommendations on how to use this command for data cleaning.
First, we suggest keeping the same variable names as in the survey instrument,
so it's easy to connect the two files.
Don't skip the labelling!
Applying labels makes it easier to understand what the data is showing while exploring the data. 
This minimizes the risk of small errors making their way through into the analysis stage.
Variable and value labels should be accurate and concise.\sidenote{\url{https://dimewiki.worldbank.org/wiki/Data\_Cleaning\#Applying\_Labels}}
Recodes should be used to turn codes for "Don't know", "Refused to answer", and
other non-responses into extended missing values.\sidenote{\url{https://dimewiki.worldbank.org/wiki/Data\_Cleaning\#Survey\_Codes\_and\_Missing\_Values}}
String variables need to be encoded, and open-ended responses, categorized or dropped\sidenote{\url{https://dimewiki.worldbank.org/wiki/Data\_Cleaning\#Strings}}
(unless you are using qualitative or classification analyses, which are less common).
Finally, any additional information collected only for quality monitoring purposes,
such as notes and duration field, can also be dropped.

% Outputs -----------------------------------------------------------------

% Data set
The most important output of data cleaning is the cleaned data set. 
It should contain the same information as the raw data set,
with no changes to data points.
It should also be easily traced back to the survey instrument,
and be accompanied by a dictionary or codebook.
Typically, one cleaned data set will be created for each data source.
Each row in the cleaned data set represents one survey entry or unit of observation.
\sidenote{\cite{tidy-data}}
If the raw data set is very large, or the survey instrument is very complex,
you may want to break the data cleaning into sub-steps, 
and create intermediate cleaned data sets
(for example, one per survey module).
Breaking cleaned data sets into the smallest unit of observation inside a roster
make the cleaning faster and the data easier to handle during construction.
But having a single cleaned data set will help you with sharing and publishing the data.
To make sure this file doesn't get too big to be handled,
use commands such as \texttt{compress} in Stata to make sure the data
is always stored in the most efficient format.

% Documentation
Throughout the data cleaning process, you will need inputs from the field, 
including enumerator manuals, survey instruments, 
supervisor notes, and data quality monitoring reports.
These materials are part of what we call data documentation
\sidenote{\url{https://dimewiki.worldbank.org/wiki/Data_Documentation}}
\index{Documentation},
and should be stored in the corresponding folder, 
as you will probably need them during analysis and publication.
Include in the \texttt{Documentation} folder records of any
corrections made to the data, including to duplicated entries,
as well as communications from the field where theses issues are reported.
Make sure to also have a record of potentially problematic patterns you noticed
while exploring the data, such as outliers and variables with many missing values.
Be very careful not to include sensitive information in 
documentation that is not securely stored, 
or that you intend to release as part of a replication package or data publication.

\section{Indicator construction}

% What is construction -------------------------------------
Any changes to the original data set happen during construction.
It is at this stage that the raw data is transformed into analysis data.
This is done by creating derived variables
(binaries, indices, and interactions, to name a few).
To understand why construction is necessary,
let's take the example of a survey's consumption module.
It will result in separate variables indicating the 
amount of each item in the bundle that was consumed.
There may be variables indicating the cost of these items.
You cannot run a meaningful regression on these variables. 
You need to manipulate them into something that has \textit{economic} meaning. 
During this process, the data points will typically be reshaped and aggregated 
so that level of the data set goes from the unit of observation in the survey to the unit of analysis. 
\sidenote{\url{https://dimewiki.worldbank.org/wiki/Unit_of_Observation}} 
To use the same example, the data on quantity consumed was collect for each item, and needs to be aggregated to the household level before analysis.

% Why it is a separate process -------------------------------

% From cleaning
Construction is done separately from data cleaning for two reasons. 
First, so you have a clear cut of what was the data originally received,
and what is the result of data processing decisions.
Second, because if you have different data sources, 
say a baseline and an endline survey, 
unless the two instruments were exactly the same, 
the data cleaning will differ between the two,
but you want to make sure that variable definition is consistent across sources. 
So you want to first merge the data sets and then create the variables only once.

% From analysis
Data construction is never a finished process.
It comes ``before'' data analysis only in a limited sense: the construction code must be run before the analysis code.
Typically, however, construction and analysis code are written concurrently.
As you write the analysis, different constructed variables will become necessary, as well as subsets and other alterations to the data.
Still, constructing variables in a separate script from the analysis will help you ensure consistency across different outputs. 
If every script that creates a table starts by loading a data set, subsetting it and manipulating variables, any edits to construction need to be replicated in all scripts. 
This increases the chances that at least one of them will have a different sample or variable definition.


% What to do during construction -----------------------------------------
Construction is the step where you face the largest risk of making a mistake that will affect your results. 
Keep in mind that details and scales matter. 
It is important to check and double-check the value-assignments of questions and their scales before constructing new variables using them. 
Are they in percentages or proportions? 
Are all variables you are combining into an index or average using the same scale? 
Are yes or no questions coded as 0 and 1, or 1 and 2?
This is when you will use the knowledge of the data you acquired and the documentation you created during the cleaning step the most.

Adding comments to the code explaining what you are doing and why is crucial here.
There are always ways for things to go wrong that you never anticipated, but two issues to pay extra attention to are missing values and dropped observations. 
If you are subsetting a data set, drop observations explicitly, indicating why you are doing that and how the data set changed.
Merging, reshaping and aggregating data sets can change both the total number of observations and the number of observations with missing values.
Make sure to read about how each command treats missing observations and, whenever possible, add automated checks in the script that throw an error message if the result is changing.

At this point, you will also need to address some of the issues in the data that you identified during data cleaning. 
The most common of them is the presence of outliers.
How to treat outliers is a research question, but make sure to note what we the decision made by the research team, and how you came to it. 
Results can be sensitive to the treatment of outliers, so keeping the original variable in the data set will allow you to test how much it affects the estimates.

% Outputs -----------------------------------------------------------------

% Data set
The outputs of construction are the data sets that will be used for analysis.
The level of observation of a constructed data set is the unit analysis. 
Each data set is purpose-built to answer an analysis question.
Since different pieces of analysis may require different samples,
or even different units of observation,
you may have one or multiple constructed data sets, 
depending on how your analysis is structured.
So don't worry if you cannot create a single, ``canonical'' analysis data set.
It is common to have many purpose-built analysis datasets:
there may be a \texttt{data-wide.dta},
\texttt{data-wide-children-only.dta}, \texttt{data-long.dta},
\texttt{data-long-counterfactual.dta}, and many more as needed.
One thing all constructed data sets should have in common, though,
are functionally-named variables.
As you no longer need to worry about keeping variable names
consistent with the survey, they should be as intuitive as possible.

% Documentation
It is wise to start an explanatory guide as soon as you start making changes to the data.
Carefully record how specific variables have been combined, recoded, and scaled. 
This can be part of a wider discussion with your team about creating protocols for variable definition.
That will guarantee that indicators are defined consistently across projects.
Documentation is an output of construction as relevant as the codes.
Someone unfamiliar with the project should be able to understand the contents of the analysis data sets, the steps taken to create them, and the decision-making process through your documentation.
The construction documentation will complement the reports and notes created during data cleaning.
Together, they will form a detailed account of the data processing.



%------------------------------------------------

\section{Writing data analysis code}

% Intro --------------------------------------------------------------
Data analysis is the stage when research outputs are created. 
\index{data analysis}
Many introductions to common code skills and analytical frameworks exist, such as
\textit{R for Data Science};\sidenote{\url{https://r4ds.had.co.nz/}}
\textit{A Practical Introduction to Stata};\sidenote{\url{https://scholar.harvard.edu/files/mcgovern/files/practical_introduction_to_stata.pdf}}
\textit{Mostly Harmless Econometrics};\sidenote{\url{https://www.researchgate.net/publication/51992844_Mostly_Harmless_Econometrics_An_Empiricist's_Companion}} 
and \textit{Causal Inference: The Mixtape}.\sidenote{\url{http://scunning.com/mixtape.html}}
This section will not include instructions on how to conduct specific analyses.
That is a research question, and requires expertise beyond the scope of this book.
Instead, we will outline the structure of writing analysis code,
assuming you have completed the process of data cleaning and construction.

% Exploratory and final data analysis -----------------------------------------
The analysis stage usually starts with a process we call exploratory data analysis.
This is when you are trying different things and looking for patterns in your data. 
It progresses into final analysis when your team starts to decide what are the main results, those that will make it into the research output.
The way you deal with code and outputs for exploratory and final analysis is different, and this section will discuss how.

% Organizing scripts ---------------------------------------------------------
During exploratory data analysis, you will be tempted to write lots of analysis into one big, impressive, start-to-finish script. 
Though it's fine to write such a script during a long analysis meeting, this practice is error-prone.  
It subtly encourages poor practices such as not clearing the workspace or loading fresh data. 
It's important to take the time to organize scripts in a clean manner and to avoid mistakes.

A well-organized analysis script starts with a completely fresh workspace and explicitly loads data before analyzing it.
This encourages data manipulation to be done earlier in the workflow (that is, during construction).
It also and prevents you from accidentally writing pieces of code that depend on one another, leading to the too-familiar ``run this part, then that part, then this part'' process.
Each script should run completely independently of all other code.
You can go as far as coding every output in a separate file.
There is nothing wrong with code files being short and simple -- as long as they directly correspond to specific pieces of analysis.

Analysis files should be as simple as possible, so you can focus on the econometrics.
All research decisions should be made very explicit in the code.
This includes clustering, sampling, and control variables, to name a few. 
If you have multiple analysis data sets, each of them should have a descriptive name about its sample and unit of observation.
As your team comes to a decision about model specification, you can create globals or objects in the master script to use across scripts.
This is a good way to make sure specifications are consistent throughout the analysis. It's also very dynamic, making it easy to update all scripts if needed.
It is completely acceptable to have folders for each task, and compartmentalize each analysis as much as needed.
It is always better to have more code files open than to keep scrolling inside a given file.

To accomplish this, you will need to make sure that you have an effective data management system, including naming, file organization, and version control.
Just like you did with each of the analysis datasets, name each of the individual analysis files descriptively.
Code files such as \path{spatial-diff-in-diff.do}, \path{matching-villages.do}, and \path{summary-statistics.do} 
are clear indicators of what each file is doing, and allow you to find code quickly.

% Self-promotion ------------------------------------------------
Out team has created a few products to automate common outputs and save you precious research time.
The \texttt{ietoolkit} package includes two commands to export nicely formatted tables.
\texttt{iebaltab}\sidenote{\url{https://dimewiki.worldbank.org/wiki/Iebaltab}} creates and exports balance tables to excel and {\TeX}. \texttt{ieddtab}\sidenote{\url{https://dimewiki.worldbank.org/wiki/Ieddtab}} does the same for difference-in-differences regressions.
The \textbf{Stata Visual Library}\sidenote{	\url{https://worldbank.github.io/Stata-IE-Visual-Library/}}
has examples of graphs created in Stata and curated by us.\sidenote{A similar resource for R is \href{https://www.r-graph-gallery.com/}{The R Graph Gallery}.}
\textbf{Data visualization}\sidenote{\url{https://dimewiki.worldbank.org/wiki/Data_visualization}} \index{data visualization}
is increasingly popular, but a great deal lacks in quality.\cite{healy2018data,wilke2019fundamentals}
We attribute some of this to the difficulty of writing code to create them.
Making a visually compelling graph would already be hard enough if you didn't have to go through many rounds of googling to understand a command.
The trickiest part of using plot commands is to get the data in the right format.
This is why the \textbf{Stata Visual Library} includes example data sets to use with each do file.

Whole books have been written on how to create good data visualizations,
so we will not attempt to give you advice on it.
Rather, here are a few resources we have found useful.
The Tapestry conference focuses on ``storytelling with data''.\sidenote{
	\url{https://www.youtube.com/playlist?list=PLb0GkPPcZCVE9EAm9qhlg5eXMgLrrfMRq}}
\textit{Fundamentals of Data Visualization} provides extensive details on practical application;\sidenote{
	\url{https://serialmentor.com/dataviz}}
as does \textit{Data Visualization: A Practical Introduction}.\sidenote{
	\url{http://socvis.co}}
Graphics tools like Stata are highly customizable.
There is a fair amount of learning curve associated with extremely-fine-grained adjustment,
but it is well worth reviewing the graphics manual\sidenote{\url{https://www.stata.com/manuals/g.pdf}}
For an easier way around it, Gray Kimbrough's \textit{Uncluttered Stata Graphs} code is an excellent default replacement for Stata graphics that is easy to install.
\sidenote{\url{https://graykimbrough.github.io/uncluttered-stata-graphs/}}
If you are a R user, the \textit{R Graphics Cookbook}\sidenote{\url{https://r-graphics.org/}} is a great resource for the most popular visualization package\texttt{ggplot}\sidenote{\url{https://ggplot2.tidyverse.org/}}. But there are a variety of other visualization packages, such as \href{http://jkunst.com/highcharter/}{\texttt{highcharter}}, \href{https://rstudio.github.io/r2d3/}{\texttt{r2d3}}, \href{https://rstudio.github.io/leaflet/}{leaflet}, and \href{https://plot.ly/r/}{plotly}, to name a few.
We have no intention of creating an exhaustive list, and this one is certainly missing very good references.
But at least it is a place to start.

\section{Exporting analysis outputs}
% Exploratory analysis
It's ok to not export each and every table and graph created during exploratory analysis. 
Instead, we suggest running them into markdown files using RMarkdown or the different dynamic document options available in Stata. 
This will allow you to update and present results quickly while maintaining a record of the different analysis tried. 
% Final analysis
Final analysis scripts, on the other hand, should export final outputs, which are ready to be included to a paper or report.
No manual edits, including formatting, should be necessary after exporting final outputs. 
Manual edits are difficult to replicate, and you will inevitably need to make changes to the outputs. 
Automating them will save you time by the end of the process. 
However, don't spend too much time formatting tables and graphs until you are ready to publish.\sidenote{For a more detailed discussion on this, including different ways to export tables from Stata, see \url{https://github.com/bbdaniels/stata-tables}}
Polishing final outputs can be a time-consuming process, 
and you want to it as few times as possible.

We cannot stress this enough: don't ever set a workflow that requires copying and pasting results from the console.
There are numerous commands to export outputs from both R and Stata.\sidenote{Some examples are \href{ http://repec.sowi.unibe.ch/stata/estout/}{\texttt{estout}}, \href{https://www.princeton.edu/~otorres/Outreg2.pdf}{\texttt{outreg2}}, and \href{https://www.benjaminbdaniels.com/stata-code/outwrite/}{\texttt{outwrite}} in Stata, and \href{https://cran.r-project.org/web/packages/stargazer/vignettes/stargazer.pdf}{\texttt{stargazer}} and \href{https://ggplot2.tidyverse.org/reference/ggsave.html}{\texttt{ggsave}} in R.}
Save outputs in accessible and, whenever possible, lightweight formats.
Accessible means that it's easy for other people to open them.
In Stata, that would mean always using \texttt{graph export} to save images as \texttt{.jpg}, \texttt{.png}, \texttt{.pdf}, etc., instead of \texttt{graph save}, which creates a \texttt{.gph} file that can only be opened through a Stata installation.
For tables, \texttt{.tex} is preferred. 
Excel \texttt{.xlsx} and \texttt{.csv} files are also acceptable, although if you are working on a large report they will become cumbersome to update after revisions.

% Formatting
If you need to create a table with a very particular format, that is not automated by any command you know, consider writing the it manually 
(Stata's \texttt{filewrite}, for example, allows you to do that).
This will allow you to write a cleaner script that focuses on the econometrics, and not on complicated commands to create and append intermediate matrices.
To avoid cluttering your scripts with formatting and ensure that formatting is consistent across outputs,
define formatting options in an R object or a Stata global and call them when needed.
% Output content
Keep in mind that final outputs should be self-standing.
This means it should be easy to read and understand them with only the information they contain.
Make sure labels and notes cover all relevant information, such as sample, unit of observation, unit of measurement and variable definition.\sidenote{ \url{https://dimewiki.worldbank.org/wiki/Checklist:\_Reviewing\_Graphs} \\ \url{https://dimewiki.worldbank.org/wiki/Checklist:\_Submit\_Table}}



%------------------------------------------------
